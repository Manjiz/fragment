# 术语

从学习 tensorflow 中摘出来记录

- ReLU：线性整流函数（Rectified Linear Unit, ReLU），又称修正线性单元，是一种人工神经网络中常用的激活函数。
- Sigmoid 函数：S 型函数，将变量映射到 0 和 1 之间。
  - tf.sigmoid
- Softmax 函数：又称归一化指数函数。它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。
- Orthogonal：正交
